{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle?scriptVersionId=1669479\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print train.shape\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.403494  0.287808  0.308698\n",
       "1  id24541  0.403494  0.287808  0.308698\n",
       "2  id00134  0.403494  0.287808  0.308698\n",
       "3  id27757  0.403494  0.287808  0.308698\n",
       "4  id04081  0.403494  0.287808  0.308698"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train.author.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 7900]\n",
      " [   1 5635]\n",
      " [   2 6044]]\n"
     ]
    }
   ],
   "source": [
    "y\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "print np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n"
     ]
    }
   ],
   "source": [
    "print (xtrain.shape)\n",
    "print (xvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 7110]\n",
      " [   1 5071]\n",
      " [   2 5440]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(ytrain, return_counts=True)\n",
    "\n",
    "print np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always start with these features. They work (almost) everytime!\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.626 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 2, 1, 2, 2, 1, 2, 1, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xvalid_ctv = ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.527 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on Counts\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [03:12, 11396.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.840B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower().decode('utf-8')\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17621/17621 [00:13<00:00, 1352.59it/s]\n",
      "100%|██████████| 1958/1958 [00:01<00:00, 1391.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00178168 -0.00567005  0.03136761 ..., -0.01590794  0.00661943\n",
      "  -0.00128377]\n",
      " [-0.00907115  0.06936043 -0.05057131 ..., -0.0010203  -0.02541611\n",
      "  -0.00590883]\n",
      " [-0.0002239  -0.0083171  -0.02808638 ...,  0.00225637  0.01857211\n",
      "  -0.0032152 ]\n",
      " ..., \n",
      " [ 0.04512988  0.00600871 -0.01274933 ...,  0.04182176 -0.02443818\n",
      "  -0.00976713]\n",
      " [ 0.015107    0.03778527 -0.06078252 ...,  0.00794491 -0.00704576\n",
      "   0.01568724]\n",
      " [ 0.00851711  0.06246101  0.01313653 ..., -0.07798822 -0.08899614\n",
      "  -0.04397095]]\n"
     ]
    }
   ],
   "source": [
    "print xtrain_glove[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1958,)\n",
      "(1958, 3)\n"
     ]
    }
   ],
   "source": [
    "print yvalid.shape\n",
    "print yvalid_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "17621/17621 [==============================] - 5s 276us/step - loss: 0.9163 - val_loss: 0.7314\n",
      "Epoch 2/5\n",
      "17621/17621 [==============================] - 5s 287us/step - loss: 0.7047 - val_loss: 0.6973\n",
      "Epoch 3/5\n",
      "17621/17621 [==============================] - 5s 306us/step - loss: 0.6469 - val_loss: 0.6695\n",
      "Epoch 4/5\n",
      "17621/17621 [==============================] - 5s 312us/step - loss: 0.5941 - val_loss: 0.6542\n",
      "Epoch 5/5\n",
      "17621/17621 [==============================] - 6s 328us/step - loss: 0.5602 - val_loss: 0.6705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a19860710>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n",
    "          epochs=5, verbose=True, \n",
    "          validation_data=(xvalid_glove_scl, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25943/25943 [00:00<00:00, 113933.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/100\n",
      "17621/17621 [==============================] - 272s 15ms/step - loss: 1.0645 - val_loss: 0.9139\n",
      "Epoch 2/100\n",
      "17621/17621 [==============================] - 260s 15ms/step - loss: 0.9094 - val_loss: 0.8068\n",
      "Epoch 3/100\n",
      "17621/17621 [==============================] - 255s 14ms/step - loss: 0.8637 - val_loss: 0.7588\n",
      "Epoch 4/100\n",
      "17621/17621 [==============================] - 249s 14ms/step - loss: 0.8258 - val_loss: 0.7321\n",
      "Epoch 5/100\n",
      "17621/17621 [==============================] - 246s 14ms/step - loss: 0.8135 - val_loss: 0.7171\n",
      "Epoch 6/100\n",
      "17621/17621 [==============================] - 250s 14ms/step - loss: 0.7865 - val_loss: 0.7111\n",
      "Epoch 7/100\n",
      "17621/17621 [==============================] - 253s 14ms/step - loss: 0.7613 - val_loss: 0.6945\n",
      "Epoch 8/100\n",
      "17621/17621 [==============================] - 256s 15ms/step - loss: 0.7513 - val_loss: 0.6666\n",
      "Epoch 9/100\n",
      "17621/17621 [==============================] - 238s 14ms/step - loss: 0.7248 - val_loss: 0.6581\n",
      "Epoch 10/100\n",
      "17621/17621 [==============================] - 254s 14ms/step - loss: 0.7069 - val_loss: 0.6271\n",
      "Epoch 11/100\n",
      "17621/17621 [==============================] - 258s 15ms/step - loss: 0.6763 - val_loss: 0.6019\n",
      "Epoch 12/100\n",
      "17621/17621 [==============================] - 241s 14ms/step - loss: 0.6574 - val_loss: 0.6005\n",
      "Epoch 13/100\n",
      "17621/17621 [==============================] - 251s 14ms/step - loss: 0.6405 - val_loss: 0.5851\n",
      "Epoch 14/100\n",
      "17621/17621 [==============================] - 254s 14ms/step - loss: 0.6225 - val_loss: 0.5750\n",
      "Epoch 15/100\n",
      "17621/17621 [==============================] - 256s 15ms/step - loss: 0.6036 - val_loss: 0.5575\n",
      "Epoch 16/100\n",
      "17621/17621 [==============================] - 273s 15ms/step - loss: 0.5860 - val_loss: 0.5401\n",
      "Epoch 17/100\n",
      "17621/17621 [==============================] - 298s 17ms/step - loss: 0.5645 - val_loss: 0.5457\n",
      "Epoch 18/100\n",
      "17621/17621 [==============================] - 249s 14ms/step - loss: 0.5488 - val_loss: 0.5263\n",
      "Epoch 19/100\n",
      "17621/17621 [==============================] - 247s 14ms/step - loss: 0.5364 - val_loss: 0.5210\n",
      "Epoch 20/100\n",
      "17621/17621 [==============================] - 251s 14ms/step - loss: 0.5149 - val_loss: 0.5092\n",
      "Epoch 21/100\n",
      "17621/17621 [==============================] - 233s 13ms/step - loss: 0.5016 - val_loss: 0.4995\n",
      "Epoch 22/100\n",
      "17621/17621 [==============================] - 198s 11ms/step - loss: 0.4922 - val_loss: 0.5153\n",
      "Epoch 23/100\n",
      "17621/17621 [==============================] - 232s 13ms/step - loss: 0.4687 - val_loss: 0.5069\n",
      "Epoch 24/100\n",
      "17621/17621 [==============================] - 260s 15ms/step - loss: 0.4614 - val_loss: 0.4912\n",
      "Epoch 25/100\n",
      "17621/17621 [==============================] - 246s 14ms/step - loss: 0.4401 - val_loss: 0.4983\n",
      "Epoch 26/100\n",
      "17621/17621 [==============================] - 247s 14ms/step - loss: 0.4292 - val_loss: 0.4900\n",
      "Epoch 27/100\n",
      "17621/17621 [==============================] - 241s 14ms/step - loss: 0.4069 - val_loss: 0.4988\n",
      "Epoch 28/100\n",
      "17621/17621 [==============================] - 214s 12ms/step - loss: 0.4154 - val_loss: 0.4821\n",
      "Epoch 29/100\n",
      "17621/17621 [==============================] - 239s 14ms/step - loss: 0.3962 - val_loss: 0.4749\n",
      "Epoch 30/100\n",
      "17621/17621 [==============================] - 27486s 2s/step - loss: 0.3834 - val_loss: 0.4996\n",
      "Epoch 31/100\n",
      "17621/17621 [==============================] - 191s 11ms/step - loss: 0.3655 - val_loss: 0.4814\n",
      "Epoch 32/100\n",
      "17621/17621 [==============================] - 188s 11ms/step - loss: 0.3538 - val_loss: 0.4985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4d2ea090>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/100\n",
      "17621/17621 [==============================] - 430s 24ms/step - loss: 1.0673 - val_loss: 0.9036\n",
      "Epoch 2/100\n",
      "17621/17621 [==============================] - 666s 38ms/step - loss: 0.9058 - val_loss: 0.7786\n",
      "Epoch 3/100\n",
      "17621/17621 [==============================] - 383s 22ms/step - loss: 0.8581 - val_loss: 0.8040\n",
      "Epoch 4/100\n",
      "17621/17621 [==============================] - 375s 21ms/step - loss: 0.8378 - val_loss: 0.7378\n",
      "Epoch 5/100\n",
      "17621/17621 [==============================] - 357s 20ms/step - loss: 0.8154 - val_loss: 0.7486\n",
      "Epoch 6/100\n",
      "17621/17621 [==============================] - 336s 19ms/step - loss: 0.7865 - val_loss: 0.6951\n",
      "Epoch 7/100\n",
      "17621/17621 [==============================] - 332s 19ms/step - loss: 0.7613 - val_loss: 0.6758\n",
      "Epoch 8/100\n",
      "17621/17621 [==============================] - 334s 19ms/step - loss: 0.7412 - val_loss: 0.6569\n",
      "Epoch 9/100\n",
      "17621/17621 [==============================] - 336s 19ms/step - loss: 0.7097 - val_loss: 0.6353\n",
      "Epoch 10/100\n",
      "17621/17621 [==============================] - 338s 19ms/step - loss: 0.6945 - val_loss: 0.6262\n",
      "Epoch 11/100\n",
      "17621/17621 [==============================] - 337s 19ms/step - loss: 0.6745 - val_loss: 0.5981\n",
      "Epoch 12/100\n",
      "17621/17621 [==============================] - 381s 22ms/step - loss: 0.6530 - val_loss: 0.5912\n",
      "Epoch 13/100\n",
      "17621/17621 [==============================] - 350s 20ms/step - loss: 0.6249 - val_loss: 0.5801\n",
      "Epoch 14/100\n",
      "17621/17621 [==============================] - 331s 19ms/step - loss: 0.6054 - val_loss: 0.5636\n",
      "Epoch 15/100\n",
      "17621/17621 [==============================] - 332s 19ms/step - loss: 0.5951 - val_loss: 0.5533\n",
      "Epoch 16/100\n",
      "17621/17621 [==============================] - 330s 19ms/step - loss: 0.5695 - val_loss: 0.5362\n",
      "Epoch 17/100\n",
      "17621/17621 [==============================] - 331s 19ms/step - loss: 0.5575 - val_loss: 0.5397\n",
      "Epoch 18/100\n",
      "17621/17621 [==============================] - 332s 19ms/step - loss: 0.5362 - val_loss: 0.5282\n",
      "Epoch 19/100\n",
      "17621/17621 [==============================] - 332s 19ms/step - loss: 0.5241 - val_loss: 0.5177\n",
      "Epoch 20/100\n",
      "17621/17621 [==============================] - 331s 19ms/step - loss: 0.5098 - val_loss: 0.5110\n",
      "Epoch 21/100\n",
      "17621/17621 [==============================] - 332s 19ms/step - loss: 0.4920 - val_loss: 0.5183\n",
      "Epoch 22/100\n",
      "17621/17621 [==============================] - 350s 20ms/step - loss: 0.4739 - val_loss: 0.4988\n",
      "Epoch 23/100\n",
      "17621/17621 [==============================] - 341s 19ms/step - loss: 0.4649 - val_loss: 0.5094\n",
      "Epoch 24/100\n",
      "17621/17621 [==============================] - 341s 19ms/step - loss: 0.4430 - val_loss: 0.5008\n",
      "Epoch 25/100\n",
      "17621/17621 [==============================] - 351s 20ms/step - loss: 0.4311 - val_loss: 0.4955\n",
      "Epoch 26/100\n",
      "17621/17621 [==============================] - 352s 20ms/step - loss: 0.4212 - val_loss: 0.5058\n",
      "Epoch 27/100\n",
      "17621/17621 [==============================] - 351s 20ms/step - loss: 0.4033 - val_loss: 0.4934\n",
      "Epoch 28/100\n",
      "17621/17621 [==============================] - 351s 20ms/step - loss: 0.3926 - val_loss: 0.5017\n",
      "Epoch 29/100\n",
      "17621/17621 [==============================] - 356s 20ms/step - loss: 0.3830 - val_loss: 0.4847\n",
      "Epoch 30/100\n",
      "17621/17621 [==============================] - 354s 20ms/step - loss: 0.3751 - val_loss: 0.5201\n",
      "Epoch 31/100\n",
      "17621/17621 [==============================] - 353s 20ms/step - loss: 0.3557 - val_loss: 0.4963\n",
      "Epoch 32/100\n",
      "17621/17621 [==============================] - 353s 20ms/step - loss: 0.3428 - val_loss: 0.5045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a623c7890>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple bidirectional LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/100\n",
      "17621/17621 [==============================] - 297s 17ms/step - loss: 1.0818 - val_loss: 0.9525\n",
      "Epoch 2/100\n",
      "17621/17621 [==============================] - 300s 17ms/step - loss: 0.9698 - val_loss: 0.8590\n",
      "Epoch 3/100\n",
      "17621/17621 [==============================] - 302s 17ms/step - loss: 0.8977 - val_loss: 0.7936\n",
      "Epoch 4/100\n",
      "17621/17621 [==============================] - 301s 17ms/step - loss: 0.8456 - val_loss: 0.7918\n",
      "Epoch 5/100\n",
      "17621/17621 [==============================] - 301s 17ms/step - loss: 0.8197 - val_loss: 0.7385\n",
      "Epoch 6/100\n",
      "17621/17621 [==============================] - 301s 17ms/step - loss: 0.7724 - val_loss: 0.6974\n",
      "Epoch 7/100\n",
      "17621/17621 [==============================] - 302s 17ms/step - loss: 0.7619 - val_loss: 0.6873\n",
      "Epoch 8/100\n",
      "17621/17621 [==============================] - 303s 17ms/step - loss: 0.7265 - val_loss: 0.6445\n",
      "Epoch 9/100\n",
      "17621/17621 [==============================] - 343s 19ms/step - loss: 0.7055 - val_loss: 0.6164\n",
      "Epoch 10/100\n",
      "17621/17621 [==============================] - 345s 20ms/step - loss: 0.6800 - val_loss: 0.6054\n",
      "Epoch 11/100\n",
      "17621/17621 [==============================] - 320s 18ms/step - loss: 0.6588 - val_loss: 0.5999\n",
      "Epoch 12/100\n",
      "17621/17621 [==============================] - 344s 19ms/step - loss: 0.6411 - val_loss: 0.5826\n",
      "Epoch 13/100\n",
      "17621/17621 [==============================] - 340s 19ms/step - loss: 0.6253 - val_loss: 0.5816\n",
      "Epoch 14/100\n",
      "17621/17621 [==============================] - 400s 23ms/step - loss: 0.6029 - val_loss: 0.5608\n",
      "Epoch 15/100\n",
      "17621/17621 [==============================] - 351s 20ms/step - loss: 0.5939 - val_loss: 0.5563\n",
      "Epoch 16/100\n",
      "17621/17621 [==============================] - 357s 20ms/step - loss: 0.5816 - val_loss: 0.5439\n",
      "Epoch 17/100\n",
      "17621/17621 [==============================] - 365s 21ms/step - loss: 0.5594 - val_loss: 0.5318\n",
      "Epoch 18/100\n",
      "17621/17621 [==============================] - 360s 20ms/step - loss: 0.5408 - val_loss: 0.5295\n",
      "Epoch 19/100\n",
      "17621/17621 [==============================] - 322s 18ms/step - loss: 0.5276 - val_loss: 0.5086\n",
      "Epoch 20/100\n",
      "17621/17621 [==============================] - 320s 18ms/step - loss: 0.5300 - val_loss: 0.5335\n",
      "Epoch 21/100\n",
      "17621/17621 [==============================] - 321s 18ms/step - loss: 0.5099 - val_loss: 0.5106\n",
      "Epoch 22/100\n",
      "17621/17621 [==============================] - 320s 18ms/step - loss: 0.4952 - val_loss: 0.5063\n",
      "Epoch 23/100\n",
      "17621/17621 [==============================] - 320s 18ms/step - loss: 0.4811 - val_loss: 0.4962\n",
      "Epoch 24/100\n",
      "17621/17621 [==============================] - 320s 18ms/step - loss: 0.4686 - val_loss: 0.4820\n",
      "Epoch 25/100\n",
      "17621/17621 [==============================] - 318s 18ms/step - loss: 0.4584 - val_loss: 0.4804\n",
      "Epoch 26/100\n",
      "17621/17621 [==============================] - 321s 18ms/step - loss: 0.4447 - val_loss: 0.4807\n",
      "Epoch 27/100\n",
      "17621/17621 [==============================] - 326s 18ms/step - loss: 0.4266 - val_loss: 0.4854\n",
      "Epoch 28/100\n",
      "17621/17621 [==============================] - 322s 18ms/step - loss: 0.4263 - val_loss: 0.4759\n",
      "Epoch 29/100\n",
      "17621/17621 [==============================] - 347s 20ms/step - loss: 0.4166 - val_loss: 0.4915\n",
      "Epoch 30/100\n",
      "17621/17621 [==============================] - 329s 19ms/step - loss: 0.4062 - val_loss: 0.4632\n",
      "Epoch 31/100\n",
      "17621/17621 [==============================] - 339s 19ms/step - loss: 0.3948 - val_loss: 0.4732\n",
      "Epoch 32/100\n",
      "17621/17621 [==============================] - 330s 19ms/step - loss: 0.3950 - val_loss: 0.4610\n",
      "Epoch 33/100\n",
      "17621/17621 [==============================] - 339s 19ms/step - loss: 0.3732 - val_loss: 0.4717\n",
      "Epoch 34/100\n",
      "17621/17621 [==============================] - 349s 20ms/step - loss: 0.3734 - val_loss: 0.4830\n",
      "Epoch 35/100\n",
      "17621/17621 [==============================] - 355s 20ms/step - loss: 0.3588 - val_loss: 0.4759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a6391e910>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRU with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   27,   29,  166],\n",
       "       [   0,    0,    0, ...,   86, 1682,  561],\n",
       "       [   0,    0,    0, ...,   27,   15,  385],\n",
       "       ..., \n",
       "       [   0,    0,    0, ...,    2,    1, 2734],\n",
       "       [   0,    0,    0, ...,  403,   12, 2136],\n",
       "       [   0,    0,    0, ...,   90,   63, 2787]], dtype=int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25943"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
